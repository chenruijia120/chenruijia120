(function(){"use strict";var e={3483:function(e,t,a){var i=a(6369),n=function(){var e=this,t=e._self._c;return t("div",{attrs:{id:"app"}},[t("router-view",[e._v("Home")])],1)},s=[],r={name:"App"},o=r,c=a(1001),l=(0,c.Z)(o,n,s,!1,null,null,null),d=l.exports,u=a(8499),h=a.n(u),p=a(2631),v=function(){var e=this,t=e._self._c;return t("div",{staticClass:"main",attrs:{id:"MyHome"}},[t("link",{attrs:{rel:"stylesheet",href:"https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css"}}),t("el-container",[t("el-main",[t("el-tabs",{key:e.activeNames,attrs:{value:e.activeNames.activeName},on:{"tab-click":e.handleClick}},[t("el-tab-pane",{attrs:{label:"MyHome",name:"first"}},[e.windowWidth>900?t("div",[t("el-row",[t("el-col",{attrs:{span:4}},[t("br"),t("el-row",[t("el-avatar",{attrs:{size:e.windowWidth/7,src:e.avatarURL}})],1),t("el-row",{attrs:{id:"icons"}},[t("a",{staticStyle:{color:"black"},attrs:{href:"mailto:ruijia.chen@wisc.edu"}},[t("i",{staticClass:"fa fa-envelope",attrs:{id:"email"}})]),t("span",[e._v("   ")]),t("a",{staticStyle:{color:"black"},attrs:{href:"https://github.com/chenruijia120",target:"_blank",rel:"noopener noreferrer"}},[t("i",{staticClass:"fa fa-github",attrs:{id:"github"}})])])],1),t("el-col",{attrs:{span:20}},[t("br"),t("div",{staticClass:"myName"},[e._v("Ruijia Chen")]),t("div",{attrs:{id:"introduction"}},[t("br"),t("p",{staticClass:"paragraph"},[e._v(" Welcome! My name is Ruijia Chen, pronounced as ray-jar (in Chinese: 陈睿嘉). ")]),t("p",{staticClass:"paragraph"},[e._v(" I am currently a second-year Ph.D. student in Computer Science at the University of Wisconsin-Madison, advised by "),t("a",{staticStyle:{color:"#9966CC"},attrs:{href:"https://www.yuhangz.com/",target:"_blank"}},[e._v("Prof. Yuhang Zhao")]),e._v(". Previously, I completed my B.E. in Computer Science at Tsinghua University, advised by "),t("a",{staticStyle:{color:"#9966CC"},attrs:{href:"https://pi.cs.tsinghua.edu.cn/lab/people/YuntaoWang/en/",target:"_blank"}},[e._v("Prof. Yuntao Wang")]),e._v(". ")]),t("p",{staticClass:"paragraph"},[e._v(" My research interests lie in "),t("span",{staticStyle:{color:"#660066","font-weight":"bolder"}},[e._v("Human-Computer Interaction")]),e._v(", especially "),t("span",{staticStyle:{color:"#660066","font-weight":"bolder"}},[e._v("augmented reality and accessibility")]),e._v(". I design and develop AR technology to automatically recognize surrounding visual information that cannot be easily perceived by people with low vision, and generate suitable multi-modal feedback to enhance their perceptual abilities in various daily tasks. ")])])])],1)],1):t("div",[t("el-row",[t("br"),t("el-row",[t("el-avatar",{attrs:{size:200,src:e.avatarURL}})],1),t("el-row",{attrs:{id:"icons"}},[t("a",{staticStyle:{color:"black"},attrs:{href:"mailto:ruijia.chen@wisc.edu"}},[t("i",{staticClass:"fa fa-envelope",attrs:{id:"email"}})]),t("span",[e._v("   ")]),t("a",{staticStyle:{color:"black"},attrs:{href:"https://github.com/chenruijia120",target:"_blank",rel:"noopener noreferrer"}},[t("i",{staticClass:"fa fa-github",attrs:{id:"github"}})])])],1),t("el-row",[t("br"),t("div",{staticClass:"myName"},[e._v("Ruijia Chen")]),t("div",{attrs:{id:"introduction"}},[t("br"),t("p",{staticClass:"paragraph"},[e._v(" Welcome! My name is Ruijia Chen, pronounced as ray-jar (in Chinese: 陈睿嘉). ")]),t("p",{staticClass:"paragraph"},[e._v(" I am currently a second-year Ph.D. student in Computer Science at the University of Wisconsin-Madison, advised by "),t("a",{staticStyle:{color:"#9966CC"},attrs:{href:"https://www.yuhangz.com/",target:"_blank"}},[e._v("Prof. Yuhang Zhao")]),e._v(". Previously, I completed my B.E. in Computer Science at Tsinghua University, advised by "),t("a",{staticStyle:{color:"#9966CC"},attrs:{href:"https://pi.cs.tsinghua.edu.cn/lab/people/YuntaoWang/en/",target:"_blank"}},[e._v("Prof. Yuntao Wang")]),e._v(". ")]),t("p",{staticClass:"paragraph"},[e._v(" My research interests lie in "),t("span",{staticStyle:{color:"#660066","font-weight":"bolder"}},[e._v("Human-Computer Interaction")]),e._v(", especially "),t("span",{staticStyle:{color:"#660066","font-weight":"bolder"}},[e._v("augmented reality and accessibility")]),e._v(". I design and develop AR technology to automatically recognize surrounding visual information that cannot be easily perceived by people with low vision, and generate suitable multi-modal feedback to enhance their perceptual abilities in various daily tasks. ")])])])],1),t("br"),t("h5",{staticClass:"section",attrs:{id:"experience-title"}},[e._v(" Experiences ")]),t("el-card",{staticClass:"box-card",attrs:{id:"experience"}},[t("p",{staticClass:"research"},[e._v(" Research Assistant | MadAbility Lab, University of Wisconsin-Madison ")]),t("p",{staticClass:"research-time"},[e._v(" August 2023 -- present ")]),t("el-divider"),t("p",{staticClass:"research"},[e._v(" Teaching Assistant for CS571 (Building User Interfaces) | University of Wisconsin-Madison ")]),t("p",{staticClass:"research-time"},[e._v(" August 2024 -- present, August 2023 -- December 2023 ")]),t("el-divider"),t("p",{staticClass:"research"},[e._v(" Research Assistant | Pervasive Interaction Lab, Department of Computer Science and Technology, Tsinghua University ")]),t("p",{staticClass:"research-time"},[e._v(" December 2020 -- August 2023 ")]),t("el-divider"),t("p",{staticClass:"research"},[e._v(" Research Assistant | University of British Columbia, Okanagan ")]),t("p",{staticClass:"research-time"},[e._v(" June 2022 -- September 2022 ")]),t("el-divider"),t("p",{staticClass:"research"},[e._v(" 2021 Access Computing Summer Program | University of Washington & Tsinghua University & Microsoft ")]),t("p",{staticClass:"research-time"},[e._v(" June 2021 -- September 2021 ")])],1),t("br"),t("br"),t("h5",{staticClass:"section",staticStyle:{color:"dimgrey","font-family":"'Trebuchet MS', 'Lucida Sans Unicode', 'Lucida Grande', 'Lucida Sans', Arial, sans-serif"}},[e._v(" Publications ")]),t("el-card",{staticClass:"box-card",attrs:{id:"publications"}},[t("p",{staticClass:"project-title"},[e._v("Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field")]),t("el-col",{attrs:{span:18}},[t("p",{staticClass:"project-conference"},[e._v("Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI '24) "),t("span",{staticStyle:{color:"grey"}},[e._v(" "),t("a",{staticStyle:{color:"grey"},attrs:{href:"https://dl.acm.org/doi/pdf/10.1145/3613904.3642195",target:"_blank"}},[e._v("[PDF]")])])]),t("p",{staticClass:"project-author"},[e._v(" Kexin Zhang, Brianna R Cochran, "),t("span",{staticStyle:{"font-weight":"bolder"}},[e._v("Ruijia Chen")]),e._v(", Lance Hartung, Bryce Sprecher, Ross Tredinnick, Kevin Ponto, Suman Banerjee, Yuhang Zhao ")]),t("div",{staticStyle:{"margin-top":"40px"}}),t("p",{staticClass:"project-content"},[e._v(" First responders (FRs) navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design space have not been thoroughly explored by involving various FRs who serve different roles (e.g., firefighters, law enforcement) but collaborate closely in the field. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed both generic and role-specific preferences and needs for AR hardware, interactions, and feedback, as well as identifying desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment. Finally, we derived comprehensive and actionable design guidelines to inform future AR systems for in-field FRs. ")])]),t("el-col",{attrs:{span:6}},[t("el-image",{attrs:{src:e.srcCHI24}})],1)],1),t("br"),t("el-card",{staticClass:"box-card",attrs:{id:"publications"}},[t("p",{staticClass:"project-title"},[e._v("Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention")]),t("el-col",{attrs:{span:19}},[t("p",{staticClass:"project-conference"},[e._v("Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT '22)    "),t("span",{staticStyle:{color:"grey"}},[t("a",{staticStyle:{color:"grey"},attrs:{href:"https://drive.google.com/file/d/15q4QfytW2d1L4mXLL7pibFZL5xWQFNT-/view?usp=sharing",target:"_blank"}},[e._v("[Video]")])]),t("span",{staticStyle:{color:"grey"}},[e._v(" "),t("a",{staticStyle:{color:"grey"},attrs:{href:"https://dl.acm.org/doi/pdf/10.1145/3534590",target:"_blank"}},[e._v("[PDF]")])])]),t("p",{staticClass:"project-author"},[e._v(" Zhipeng Li, Yu Jiang, Yihao Zhu, "),t("span",{staticStyle:{"font-weight":"bolder"}},[e._v("Ruijia Chen")]),e._v(", Ruolin Wang, Yuntao Wang, Yukang Yan, Yuanchun Shi ")]),t("p",{staticClass:"project-content"},[e._v(" We applied angular offsets to the avatar's shoulder and elbow joints, recorded users' responses and identifications to the inconsistency through a three-stage user studies, and built a statistical model based on the results. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). We demonstrated the model's extendibility in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement. ")])]),t("el-col",{attrs:{span:5}},[t("el-image",{attrs:{src:e.srcIMWUT}})],1)],1),t("br"),t("br")],1),t("el-tab-pane",{attrs:{label:"CV",name:"third"}})],1)],1),t("el-backtop",{attrs:{right:100,bottom:100}})],1)],1)},f=[],g={name:"MyHome",data(){return{activeNames:{activeName:"first",lastActiveName:"first"},avatarURL:a(4447),srcIMWUT:a(1234),srcCHI24:a(5503),windowWidth:document.documentElement.clientWidth,windowHeight:document.documentElement.clientHeight}},methods:{handleClick(e,t){console.log(e.name,t),"third"==e.name?(window.open("https://drive.google.com/file/d/11cAWXHRW2KdKBYTmvCHM6iibuO_TeQDV/view?usp=drive_link","_blank"),this.$set(this.activeNames,"activeName",this.activeNames.lastActiveName),this.activeNames=Object.assign({},this.activeNames,{activeName:this.activeNames.lastActiveName,lastActiveName:this.activeNames.lastActiveName})):(console.log(this.lastActiveName,this.activeName),this.activeNames.lastActiveName=e.name),console.log(this.activeNames.lastActiveName,this.activeNames.activeName)}},mounted(){var e=this;window.onresize=()=>(()=>{window.fullHeight=document.documentElement.clientHeight,window.fullWidth=document.documentElement.clientWidth,e.windowHeight=window.fullHeight,e.windowWidth=window.fullWidth})()},watch:{windowHeight(e){let t=this;console.log("实时屏幕高度：",e,t.windowHeight)},windowWidth(e){let t=this;console.log("实时屏幕宽度：",e,t.windowHeight)}}},m=g,y=(0,c.Z)(m,v,f,!1,null,null,null),b=y.exports;i["default"].use(p.ZP);const w=new p.ZP({mode:"history",routes:[{path:"/",name:"MyHome",component:b,meta:{title:"Ruijia Chen"}},{path:"*",redirect:"/"}]});var C=w;i["default"].use(h()),new i["default"]({router:C,render:e=>e(d)}).$mount("#app")},5503:function(e,t,a){e.exports=a.p+"img/CHI2024.34d863a4.jpg"},1234:function(e,t,a){e.exports=a.p+"img/IMWUT2022.6af4e3a0.png"},4447:function(e,t,a){e.exports=a.p+"img/photo.68bca30c.jpg"}},t={};function a(i){var n=t[i];if(void 0!==n)return n.exports;var s=t[i]={id:i,loaded:!1,exports:{}};return e[i].call(s.exports,s,s.exports,a),s.loaded=!0,s.exports}a.m=e,function(){a.amdO={}}(),function(){var e=[];a.O=function(t,i,n,s){if(!i){var r=1/0;for(d=0;d<e.length;d++){i=e[d][0],n=e[d][1],s=e[d][2];for(var o=!0,c=0;c<i.length;c++)(!1&s||r>=s)&&Object.keys(a.O).every((function(e){return a.O[e](i[c])}))?i.splice(c--,1):(o=!1,s<r&&(r=s));if(o){e.splice(d--,1);var l=n();void 0!==l&&(t=l)}}return t}s=s||0;for(var d=e.length;d>0&&e[d-1][2]>s;d--)e[d]=e[d-1];e[d]=[i,n,s]}}(),function(){a.n=function(e){var t=e&&e.__esModule?function(){return e["default"]}:function(){return e};return a.d(t,{a:t}),t}}(),function(){a.d=function(e,t){for(var i in t)a.o(t,i)&&!a.o(e,i)&&Object.defineProperty(e,i,{enumerable:!0,get:t[i]})}}(),function(){a.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(e){if("object"===typeof window)return window}}()}(),function(){a.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)}}(),function(){a.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})}}(),function(){a.nmd=function(e){return e.paths=[],e.children||(e.children=[]),e}}(),function(){a.p="/"}(),function(){var e={143:0};a.O.j=function(t){return 0===e[t]};var t=function(t,i){var n,s,r=i[0],o=i[1],c=i[2],l=0;if(r.some((function(t){return 0!==e[t]}))){for(n in o)a.o(o,n)&&(a.m[n]=o[n]);if(c)var d=c(a)}for(t&&t(i);l<r.length;l++)s=r[l],a.o(e,s)&&e[s]&&e[s][0](),e[s]=0;return a.O(d)},i=self["webpackChunkchenruijia120"]=self["webpackChunkchenruijia120"]||[];i.forEach(t.bind(null,0)),i.push=t.bind(null,i.push.bind(i))}();var i=a.O(void 0,[998],(function(){return a(3483)}));i=a.O(i)})();
//# sourceMappingURL=app.d274b3a8.js.map